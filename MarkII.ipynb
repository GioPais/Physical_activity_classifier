{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 3: Redes Neuronales Artificiales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importación de DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "mean:[0.027909902,-0.99785272,-0.001931412,1.506938,1.265255174,-0.94328015,-25.469129999999996,-42.301207999999995,3.738514626]\n",
      "var:[0.005201803730236396,0.009767228441801602,0.005743996674838256,0.002185380396,1.8675492931219797,1.5136131910446715,1.4289264010999991,0.3879815887359995,7.833129032443055]\n",
      "H:[-inf,6.209660310234993,-inf,6.214115146148642,-inf,-inf,6.213506664555782,6.214499757896755,-inf]\n",
      "K:[2.0797710132964573,0.1562451071595672,0.5480248396315996,5.056218202195623,0.11687826738541052,-1.1937164304285248,0.6256594305287222,1.8258631761652477,-0.6564941703619174]\n",
      "EE:[1.0038016560590113,2.671732106662758,49.61547449898034]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn as skl\n",
    "import scipy as sci\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "DIR='C:\\Projects\\Physical_activity_classifier/Smartphone_Dataset'\n",
    "df= pd.read_csv(DIR+'/S01/bike1.csv',header=None)\n",
    "print(type(df))\n",
    "data=df.as_matrix()\n",
    "d_size=len(data)\n",
    "#print(data)\n",
    "final_data=[]\n",
    "#Extract data for sensor\n",
    "sensors=[[],[],[],[],[],[],[],[],[]]\n",
    "for i in range(0,d_size):\n",
    "    for j in range(0,9):\n",
    "        sensors[j].append(data[i][j])\n",
    "\n",
    "#---------------------mean-----------------------\n",
    "ms=[]\n",
    "for j in range(0,9):\n",
    "    mean=np.mean(sensors[j])\n",
    "    final_data.append(mean)\n",
    "    ms.append(mean)\n",
    "print('mean:[{},{},{},{},{},{},{},{},{}]'.format(ms[0],ms[1],ms[2],ms[3],ms[4],ms[5],ms[6],ms[7],ms[8]))\n",
    "#---------------------Var------------------------    \n",
    "vs=[]\n",
    "for j in range(0,9):\n",
    "    var=np.var(sensors[j])\n",
    "    final_data.append(var)\n",
    "    vs.append(var)\n",
    "print('var:[{},{},{},{},{},{},{},{},{}]'.format(vs[0],vs[1],vs[2],vs[3],vs[4],vs[5],vs[6],vs[7],vs[8]))\n",
    "#-------------------Entropy----------------------\n",
    "Hs=[]\n",
    "for j in range(0,9):\n",
    "    H=sci.stats.entropy(sensors[j])\n",
    "    #final_data.append(H)\n",
    "    Hs.append(H)\n",
    "print('H:[{},{},{},{},{},{},{},{},{}]'.format(Hs[0],Hs[1],Hs[2],Hs[3],Hs[4],Hs[5],Hs[6],Hs[7],Hs[8]))\n",
    "#-------------------Kurtosis----------------------\n",
    "Ks=[]\n",
    "for j in range(0,9):\n",
    "    K=sci.stats.kurtosis(sensors[j])\n",
    "    final_data.append(K)\n",
    "    Ks.append(K)\n",
    "print('K:[{},{},{},{},{},{},{},{},{}]'.format(Ks[0],Ks[1],Ks[2],Ks[3],Ks[4],Ks[5],Ks[6],Ks[7],Ks[8]))\n",
    "#-------------------Kurtosis----------------------\n",
    "EEs=[]\n",
    "for j in range(0,3):\n",
    "    EE=0\n",
    "    for i in range(0,d_size):\n",
    "        EE+=(1/d_size)*np.sqrt(sensors[0+j*3][i]*sensors[0+j*3][i]+sensors[1+j*3][i]*sensors[1+j*3][i]+sensors[2+j*3][i]*sensors[2+j*3][i])\n",
    "    #EE=sci.stats.kurtosis(sensors[j])\n",
    "    #final_data.append(K)\n",
    "    EEs.append(EE)\n",
    "print('EE:[{},{},{}]'.format(EEs[0],EEs[1],EEs[2]))\n",
    "    \n",
    "#print(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Projects\\Physical_activity_classifier/Smartphone_Dataset/S02/gymbike1.csv path doesn t exist\n",
      "C:\\Projects\\Physical_activity_classifier/Smartphone_Dataset/S02/gymbike2.csv path doesn t exist\n",
      "C:\\Projects\\Physical_activity_classifier/Smartphone_Dataset/S02/gymbike3.csv path doesn t exist\n",
      "C:\\Projects\\Physical_activity_classifier/Smartphone_Dataset/S02/gymbike4.csv path doesn t exist\n",
      "C:\\Projects\\Physical_activity_classifier/Smartphone_Dataset/S02/gymbike5.csv path doesn t exist\n",
      "C:\\Projects\\Physical_activity_classifier/Smartphone_Dataset/S03/treadmill3.csv path doesn t exist\n",
      "C:\\Projects\\Physical_activity_classifier/Smartphone_Dataset/S04/bike1.csv path doesn t exist\n",
      "C:\\Projects\\Physical_activity_classifier/Smartphone_Dataset/S04/bike2.csv path doesn t exist\n",
      "C:\\Projects\\Physical_activity_classifier/Smartphone_Dataset/S04/bike3.csv path doesn t exist\n",
      "C:\\Projects\\Physical_activity_classifier/Smartphone_Dataset/S04/bike4.csv path doesn t exist\n",
      "C:\\Projects\\Physical_activity_classifier/Smartphone_Dataset/S04/bike5.csv path doesn t exist\n",
      "C:\\Projects\\Physical_activity_classifier/Smartphone_Dataset/S06/bike1.csv path doesn t exist\n",
      "C:\\Projects\\Physical_activity_classifier/Smartphone_Dataset/S06/bike2.csv path doesn t exist\n",
      "C:\\Projects\\Physical_activity_classifier/Smartphone_Dataset/S06/bike3.csv path doesn t exist\n",
      "C:\\Projects\\Physical_activity_classifier/Smartphone_Dataset/S06/bike4.csv path doesn t exist\n",
      "C:\\Projects\\Physical_activity_classifier/Smartphone_Dataset/S06/bike5.csv path doesn t exist\n",
      "C:\\Projects\\Physical_activity_classifier/Smartphone_Dataset/S07/bike1.csv path doesn t exist\n",
      "C:\\Projects\\Physical_activity_classifier/Smartphone_Dataset/S07/bike2.csv path doesn t exist\n",
      "C:\\Projects\\Physical_activity_classifier/Smartphone_Dataset/S07/bike3.csv path doesn t exist\n",
      "C:\\Projects\\Physical_activity_classifier/Smartphone_Dataset/S07/bike4.csv path doesn t exist\n",
      "C:\\Projects\\Physical_activity_classifier/Smartphone_Dataset/S07/bike5.csv path doesn t exist\n",
      "C:\\Projects\\Physical_activity_classifier/Smartphone_Dataset/S07/gymbike5.csv path doesn t exist\n",
      "Cantidad de datos: 383\n",
      "Cantidad de caracteristicas: 30\n",
      "[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 7, 7, 7, 7, 8, 8, 8, 8, 8, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as skl\n",
    "import numpy as np\n",
    "import scipy as sci\n",
    "\n",
    "DIR='C:\\Projects\\Physical_activity_classifier/Smartphone_Dataset'\n",
    "data= pd.read_csv(DIR+'/S01/bike1.csv',header=None)\n",
    "#print(data)\n",
    "\n",
    "\n",
    "\n",
    "x=[]\n",
    "y=[]\n",
    "for suj in range(1,10):#######################10\n",
    "    for act in range(0,9):###################9\n",
    "        Activities=['bike','climbing','descending','gymbike','jumping','running','standing','treadmill','walking']\n",
    "        for k in range(1,6):\n",
    "            path=DIR+'/S0'+str(suj)+'/'+Activities[act]+str(k)+'.csv'\n",
    "            try:\n",
    "                #Activity[j].append(pd.read_csv(path,header=None))\n",
    "                df=pd.read_csv(path,header=None)\n",
    "                \n",
    "                #print(type(df))\n",
    "                data=df.as_matrix()\n",
    "                d_size=len(data)\n",
    "                #print(data)\n",
    "                final_data=[]\n",
    "                #Extract data for sensor\n",
    "                sensors=[[],[],[],[],[],[],[],[],[]]\n",
    "                for i in range(0,d_size):\n",
    "                    for j in range(0,9):\n",
    "                        sensors[j].append(data[i][j])\n",
    "\n",
    "                #---------------------mean-----------------------\n",
    "                ms=[]\n",
    "                for j in range(0,9):\n",
    "                    mean=np.mean(sensors[j])\n",
    "                    final_data.append(mean)\n",
    "                    ms.append(mean)\n",
    "                #print('mean:[{},{},{},{},{},{},{},{},{}]'.format(ms[0],ms[1],ms[2],ms[3],ms[4],ms[5],ms[6],ms[7],ms[8]))\n",
    "                #---------------------Var------------------------    \n",
    "                vs=[]\n",
    "                for j in range(0,9):\n",
    "                    var=np.var(sensors[j])\n",
    "                    final_data.append(var)\n",
    "                    vs.append(var)\n",
    "                #print('var:[{},{},{},{},{},{},{},{},{}]'.format(vs[0],vs[1],vs[2],vs[3],vs[4],vs[5],vs[6],vs[7],vs[8]))\n",
    "                #-------------------Entropy----------------------\n",
    "                Hs=[]\n",
    "                for j in range(0,9):\n",
    "                    H=sci.stats.entropy(sensors[j])\n",
    "                    #if(H==1):\n",
    "                    #    print('hola')\n",
    "                    #final_data.append(H)\n",
    "                    Hs.append(H)\n",
    "                #print('H:[{},{},{},{},{},{},{},{},{}]'.format(Hs[0],Hs[1],Hs[2],Hs[3],Hs[4],Hs[5],Hs[6],Hs[7],Hs[8]))\n",
    "                #-------------------Kurtosis----------------------\n",
    "                Ks=[]\n",
    "                for j in range(0,9):\n",
    "                    K=sci.stats.kurtosis(sensors[j])\n",
    "                    final_data.append(K)\n",
    "                    Ks.append(K)\n",
    "                #print('K:[{},{},{},{},{},{},{},{},{}]'.format(Ks[0],Ks[1],Ks[2],Ks[3],Ks[4],Ks[5],Ks[6],Ks[7],Ks[8]))\n",
    "                #-------------------Kurtosis----------------------\n",
    "                EEs=[]\n",
    "                for j in range(0,3):\n",
    "                    EE=0\n",
    "                    for i in range(0,d_size):\n",
    "                        EE+=(1/d_size)*np.sqrt(sensors[0+j*3][i]*sensors[0+j*3][i]+sensors[1+j*3][i]*sensors[1+j*3][i]+sensors[2+j*3][i]*sensors[2+j*3][i])\n",
    "                    #EE=sci.stats.kurtosis(sensors[j])\n",
    "                    final_data.append(K)\n",
    "                    EEs.append(EE)\n",
    "                #print('K:[{},{},{}]'.format(EEs[0],EEs[1],EEs[2]))\n",
    "                \n",
    "                #-------------------Kurtosis----------------------\n",
    "                RMSs=[]\n",
    "                for j in range(0,9):\n",
    "                    RMS=0\n",
    "                    for i in range(0,d_size):\n",
    "                        RMS+=np.sqrt((1/d_size)*sensors[j][i]*sensors[j][i])\n",
    "                    #EE=sci.stats.kurtosis(sensors[j])\n",
    "                    #final_data.append(RMS)\n",
    "                    RMSs.append(RMS)\n",
    "                #print('RMS:[{},{},{}]'.format(RMSs[0],RMSs[1],RMSs[2]))\n",
    "                \n",
    "                y.append(act)\n",
    "                #print(final_data)\n",
    "                x.append(final_data)\n",
    "                \n",
    "            except FileNotFoundError :\n",
    "                print(path+' path doesn t exist')\n",
    "        \n",
    "    \n",
    "print('Cantidad de datos: {}'.format(len(x)))\n",
    "print('Cantidad de caracteristicas: {}'.format(len(x[0])))\n",
    "#print('Datos por actividad: [{},{},{},{},{},{},{},{},{}]'.format(len(Activity[0]),len(Activity[1]),len(Activity[2]),len(Activity[3]),len(Activity[4]),len(Activity[5]),len(Activity[6]),len(Activity[7]),len(Activity[8])))\n",
    "xtrain=x\n",
    "ytrain=y\n",
    "xval=[]\n",
    "yval=[]\n",
    "print(ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "def ANN_train(xdata,ydata,hl_size=10,activation='relu'):\n",
    "    clf = MLPClassifier(activation=activation,solver='lbfgs', hidden_layer_sizes=hl_size,\n",
    "    early_stopping = True, validation_fraction = 0.20);\n",
    "    #X, X_val, y, y_val = train_test_split(xdata, np.ravel(ydata), test_size=clf.validation_fraction);\n",
    "    \n",
    "    xtrain,xval,ytrain,yval=train_test_split(xdata,ydata,random_state=clf.random_state,\n",
    "                                                       test_size=clf.validation_fraction)\n",
    "    \n",
    "    #print(y_val)\n",
    "    #print('size: {}'.format(len(y_val)))\n",
    "    \n",
    "    start = time.clock()\n",
    "    clf.fit(xtrain, np.ravel(ytrain))\n",
    "    end = time.clock()\n",
    "    dt=end-start\n",
    "    return [clf,xtrain,xval,ytrain,yval,dt]\n",
    "    \n",
    "\n",
    "def ANN_perf(xtest,ytest,clf):\n",
    "    ypred=clf.predict(xtest)\n",
    "    c_m=confusion_matrix(ytest, ypred)\n",
    "    return c_m\n",
    "\n",
    "def get_D(c_m):\n",
    "    dsum=0\n",
    "    for i in range(0,len(c_m)):\n",
    "        dsum+=c_m[i][i]\n",
    "    return dsum\n",
    "\n",
    "def get_Darray(c_m):\n",
    "    D=[]\n",
    "    for i in xrange(0,len(c_m)):\n",
    "        D.append(c_m[i][i])\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Neural Network\n",
      "-------------------------------------------------\n",
      "Matriz de Confusión para 20 neuronas\n",
      "[[5 2 0 0 0 0 0 0 1]\n",
      " [0 5 0 0 0 0 0 0 0]\n",
      " [1 1 9 1 0 1 0 0 0]\n",
      " [2 0 1 7 0 0 0 0 1]\n",
      " [0 0 0 0 5 1 0 0 0]\n",
      " [0 0 0 0 2 5 0 0 0]\n",
      " [0 0 0 0 0 0 5 0 0]\n",
      " [0 1 0 0 1 1 0 7 0]\n",
      " [0 0 2 0 1 0 1 0 8]]\n",
      "Clasificaciones exitosas: 56 de 77(72.7273%)\n",
      "Tiempo de entrenamiento: 0.4348126956319902s\n",
      "------------------------------------------------\n",
      "Matriz de Confusión para 500 neuronas\n",
      "[[ 4  0  0  0  0  0  0  0  0]\n",
      " [ 0  3  1  0  0  0  0  0  0]\n",
      " [ 0  1  8  1  0  0  0  0  0]\n",
      " [ 1  0  0  6  0  0  0  0  0]\n",
      " [ 1  0  0  0  6  2  0  0  1]\n",
      " [ 0  0  0  0  1  7  0  1  1]\n",
      " [ 1  0  1  2  0  0  9  0  0]\n",
      " [ 0  0  0  0  0  0  0 12  0]\n",
      " [ 1  0  1  0  0  0  0  1  4]]\n",
      "Clasificaciones exitosas: 59 de 77(76.6234%)\n",
      "Tiempo de entrenamiento: 4.050209311026265s\n",
      "------------------------------------------------\n",
      "Matriz de Confusión para 1000 neuronas\n",
      "[[10  0  0  0  0  0  0  0  0]\n",
      " [ 0  5  0  0  0  0  0  0  0]\n",
      " [ 0  1  8  0  0  0  0  0  1]\n",
      " [ 0  0  0  3  0  0  0  0  1]\n",
      " [ 0  0  0  0  5  3  0  0  0]\n",
      " [ 0  0  0  0  3  4  0  0  0]\n",
      " [ 0  0  0  0  0  0  9  0  0]\n",
      " [ 0  0  0  0  1  0  0 13  1]\n",
      " [ 0  0  1  0  1  1  0  0  6]]\n",
      "Clasificaciones exitosas: 63 de 77(81.8182%)\n",
      "Tiempo de entrenamiento: 8.081659074814525s\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#hl_size_op=5000#(28+10)//2\n",
    "hl_s=[20,500,1000]\n",
    "\n",
    "print('-------------------------------------------------')\n",
    "print('Neural Network')\n",
    "print('-------------------------------------------------')\n",
    "\n",
    "for i in range(0,3):\n",
    "    clf,xtrain,xval,ytrain,yval,dt=ANN_train(x,y,hl_size=hl_s[i])\n",
    "    \n",
    "    c_m=ANN_perf(xval,yval,clf)\n",
    "    print('Matriz de Confusión para {} neuronas'.format(hl_s[i]))\n",
    "    print(c_m)\n",
    "    dsum=get_D(c_m)\n",
    "    percentage=np.round((dsum/(len(yval)*1.0))*100,4)\n",
    "    print('Clasificaciones exitosas: {} de {}({}%)'.format(dsum,len(yval),percentage))\n",
    "    print('Tiempo de entrenamiento: {}s'.format(dt))\n",
    "    print('------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Support Vector Machine\n",
      "-------------------------------------------------\n",
      "Matriz de Confusión:\n",
      "[[ 0 10  0  0  0  0  0  0  0]\n",
      " [ 0  5  0  0  0  0  0  0  0]\n",
      " [ 0 10  0  0  0  0  0  0  0]\n",
      " [ 0  4  0  0  0  0  0  0  0]\n",
      " [ 0  8  0  0  0  0  0  0  0]\n",
      " [ 0  7  0  0  0  0  0  0  0]\n",
      " [ 0  9  0  0  0  0  0  0  0]\n",
      " [ 0 15  0  0  0  0  0  0  0]\n",
      " [ 0  9  0  0  0  0  0  0  0]]\n",
      "Clasificaciones exitosas: 5 de 77(6.4935%)\n",
      "-------------------------------------------------\n",
      "Matriz de Confusión:\n",
      "[[6 1 2 0 1 0 0 0 0]\n",
      " [0 5 0 0 0 0 0 0 0]\n",
      " [0 1 8 0 0 0 0 0 1]\n",
      " [2 0 2 0 0 0 0 0 0]\n",
      " [0 5 0 0 3 0 0 0 0]\n",
      " [0 1 2 0 2 0 0 1 1]\n",
      " [1 0 4 0 0 0 4 0 0]\n",
      " [0 5 3 0 0 0 0 7 0]\n",
      " [0 2 1 0 1 0 0 3 2]]\n",
      "Clasificaciones exitosas: 35 de 77(45.4545%)\n",
      "-------------------------------------------------\n",
      "Matriz de Confusión:\n",
      "[[4 1 3 0 2 0 0 0 0]\n",
      " [0 5 0 0 0 0 0 0 0]\n",
      " [0 1 8 0 0 0 0 0 1]\n",
      " [2 0 2 0 0 0 0 0 0]\n",
      " [0 5 0 0 3 0 0 0 0]\n",
      " [0 1 2 0 3 0 0 1 0]\n",
      " [2 0 6 0 0 0 0 1 0]\n",
      " [0 5 2 0 0 0 0 8 0]\n",
      " [0 2 2 0 2 0 0 3 0]]\n",
      "Clasificaciones exitosas: 28 de 77(36.3636%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC,LinearSVC\n",
    "def svm_poly(xtrain,ytrain,xtest,ytest,deg=3):\n",
    "    #Entrenamiento\n",
    "    std_scale=skl.preprocessing.StandardScaler().fit(xtrain)\n",
    "    df_std = std_scale.transform(xtrain)\n",
    "    clf = SVC(kernel='linear',C=deg)\n",
    "    clf.fit(df_std, ytrain)  \n",
    "    #Validacion\n",
    "    std_scale=skl.preprocessing.StandardScaler().fit(xtest)\n",
    "    feattest = std_scale.transform(xtest)\n",
    "    #score = clf_p.decision_function(feattest)\n",
    "    #ypred=clf.predict(feattest)\n",
    "    \n",
    "    c_m=ANN_perf(xtest,ytest,clf)\n",
    "    print('Matriz de Confusión:')\n",
    "    print(c_m)\n",
    "    dsum=get_D(c_m)\n",
    "    percentage=np.round((dsum/(len(ytest)*1.0))*100,4)\n",
    "    print('Clasificaciones exitosas: {} de {}({}%)'.format(dsum,len(ytest),percentage))\n",
    "    #print('Tiempo de entrenamiento: {}s'.format(result[5]))\n",
    "    \n",
    "print('-------------------------------------------------')\n",
    "print('Support Vector Machine')\n",
    "print('-------------------------------------------------')\n",
    "svm_poly(xtrain,ytrain,xval,yval,deg=0.0001)\n",
    "print('-------------------------------------------------')\n",
    "svm_poly(xtrain,ytrain,xval,yval,deg=1)\n",
    "print('-------------------------------------------------')\n",
    "svm_poly(xtrain,ytrain,xval,yval,deg=1000)\n",
    "#svm_poly(Mtrain,Mval,deg=2)\n",
    "#svm_poly(Mtrain,Mval,deg=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Gaussian Naive Bayes\n",
      "-------------------------------------------------\n",
      "Matriz de Confusión:\n",
      "[[ 0  1  0  0  0  9  0  0  0]\n",
      " [ 0  5  0  0  0  0  0  0  0]\n",
      " [ 0  3  3  0  0  4  0  0  0]\n",
      " [ 0  0  0  0  0  4  0  0  0]\n",
      " [ 0  1  1  0  0  6  0  0  0]\n",
      " [ 0  1  0  0  0  6  0  0  0]\n",
      " [ 0  0  0  0  6  3  0  0  0]\n",
      " [ 0  1  1  0  1 12  0  0  0]\n",
      " [ 0  2  0  0  1  6  0  0  0]]\n",
      "Clasificaciones exitosas: 14 de 77(18.1818%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "std_scale=skl.preprocessing.StandardScaler().fit(xtrain)\n",
    "df_std = std_scale.transform(xtrain)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(df['Consumer_complaint_narrative'], df['Product'], random_state = 0)\n",
    "#count_vect = CountVectorizer()\n",
    "#X_train_counts = count_vect.fit_transform(X_train)\n",
    "#tfidf_transformer = TfidfTransformer()\n",
    "#X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "clf = GaussianNB().fit(df_std, ytrain)\n",
    "\n",
    "#xtest=xtrain\n",
    "#ytest=ytrain\n",
    "\n",
    "std_scale=skl.preprocessing.StandardScaler().fit(xval)\n",
    "feattest = std_scale.transform(xval)\n",
    "#score = clf_p.decision_function(feattest)\n",
    "#ypred=clf.predict(feattest)\n",
    "\n",
    "c_m=ANN_perf(xval,yval,clf)\n",
    "\n",
    "print('-------------------------------------------------')\n",
    "print('Gaussian Naive Bayes')\n",
    "print('-------------------------------------------------')\n",
    "\n",
    "print('Matriz de Confusión:')\n",
    "print(c_m)\n",
    "dsum=get_D(c_m)\n",
    "percentage=np.round((dsum/(len(yval)*1.0))*100,4)\n",
    "print('Clasificaciones exitosas: {} de {}({}%)'.format(dsum,len(yval),percentage))\n",
    "#print('Tiempo de entrenamiento: {}s'.format(result[5]))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
