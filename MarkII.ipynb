{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 3: Redes Neuronales Artificiales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importación de DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "mean:[0.027909902,-0.99785272,-0.001931412,1.506938,1.265255174,-0.94328015,-25.469129999999996,-42.301207999999995,3.738514626]\n",
      "var:[0.005201803730236396,0.009767228441801602,0.005743996674838256,0.002185380396,1.8675492931219797,1.5136131910446715,1.4289264010999991,0.3879815887359995,7.833129032443055]\n",
      "H:[-inf,6.209660310234993,-inf,6.214115146148642,-inf,-inf,6.213506664555782,6.214499757896755,-inf]\n",
      "K:[2.0797710132964573,0.1562451071595672,0.5480248396315996,5.056218202195623,0.11687826738541052,-1.1937164304285248,0.6256594305287222,1.8258631761652477,-0.6564941703619174]\n",
      "EE:[1.0038016560590113,2.671732106662758,49.61547449898034]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn as skl\n",
    "import scipy as sci\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "DIR='C:\\Projects\\Physical_activity_classifier/Smartphone_Dataset'\n",
    "df= pd.read_csv(DIR+'/S01/bike1.csv',header=None)\n",
    "print(type(df))\n",
    "data=df.as_matrix()\n",
    "d_size=len(data)\n",
    "#print(data)\n",
    "final_data=[]\n",
    "#Extract data for sensor\n",
    "sensors=[[],[],[],[],[],[],[],[],[]]\n",
    "for i in range(0,d_size):\n",
    "    for j in range(0,9):\n",
    "        sensors[j].append(data[i][j])\n",
    "\n",
    "#---------------------mean-----------------------\n",
    "ms=[]\n",
    "for j in range(0,9):\n",
    "    mean=np.mean(sensors[j])\n",
    "    final_data.append(mean)\n",
    "    ms.append(mean)\n",
    "print('mean:[{},{},{},{},{},{},{},{},{}]'.format(ms[0],ms[1],ms[2],ms[3],ms[4],ms[5],ms[6],ms[7],ms[8]))\n",
    "#---------------------Var------------------------    \n",
    "vs=[]\n",
    "for j in range(0,9):\n",
    "    var=np.var(sensors[j])\n",
    "    final_data.append(var)\n",
    "    vs.append(var)\n",
    "print('var:[{},{},{},{},{},{},{},{},{}]'.format(vs[0],vs[1],vs[2],vs[3],vs[4],vs[5],vs[6],vs[7],vs[8]))\n",
    "#-------------------Entropy----------------------\n",
    "Hs=[]\n",
    "for j in range(0,9):\n",
    "    H=sci.stats.entropy(sensors[j])\n",
    "    #final_data.append(H)\n",
    "    Hs.append(H)\n",
    "print('H:[{},{},{},{},{},{},{},{},{}]'.format(Hs[0],Hs[1],Hs[2],Hs[3],Hs[4],Hs[5],Hs[6],Hs[7],Hs[8]))\n",
    "#-------------------Kurtosis----------------------\n",
    "Ks=[]\n",
    "for j in range(0,9):\n",
    "    K=sci.stats.kurtosis(sensors[j])\n",
    "    final_data.append(K)\n",
    "    Ks.append(K)\n",
    "print('K:[{},{},{},{},{},{},{},{},{}]'.format(Ks[0],Ks[1],Ks[2],Ks[3],Ks[4],Ks[5],Ks[6],Ks[7],Ks[8]))\n",
    "#-------------------Kurtosis----------------------\n",
    "EEs=[]\n",
    "for j in range(0,3):\n",
    "    EE=0\n",
    "    for i in range(0,d_size):\n",
    "        EE+=(1/d_size)*np.sqrt(sensors[0+j*3][i]*sensors[0+j*3][i]+sensors[1+j*3][i]*sensors[1+j*3][i]+sensors[2+j*3][i]*sensors[2+j*3][i])\n",
    "    #EE=sci.stats.kurtosis(sensors[j])\n",
    "    #final_data.append(K)\n",
    "    EEs.append(EE)\n",
    "print('EE:[{},{},{}]'.format(EEs[0],EEs[1],EEs[2]))\n",
    "    \n",
    "#print(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Projects\\Physical_activity_classifier/Smartphone_Dataset/S02/gymbike1.csv path doesn t exist\n",
      "C:\\Projects\\Physical_activity_classifier/Smartphone_Dataset/S02/gymbike2.csv path doesn t exist\n",
      "C:\\Projects\\Physical_activity_classifier/Smartphone_Dataset/S02/gymbike3.csv path doesn t exist\n",
      "C:\\Projects\\Physical_activity_classifier/Smartphone_Dataset/S02/gymbike4.csv path doesn t exist\n",
      "C:\\Projects\\Physical_activity_classifier/Smartphone_Dataset/S02/gymbike5.csv path doesn t exist\n",
      "C:\\Projects\\Physical_activity_classifier/Smartphone_Dataset/S03/treadmill3.csv path doesn t exist\n",
      "C:\\Projects\\Physical_activity_classifier/Smartphone_Dataset/S04/bike1.csv path doesn t exist\n",
      "C:\\Projects\\Physical_activity_classifier/Smartphone_Dataset/S04/bike2.csv path doesn t exist\n",
      "C:\\Projects\\Physical_activity_classifier/Smartphone_Dataset/S04/bike3.csv path doesn t exist\n",
      "C:\\Projects\\Physical_activity_classifier/Smartphone_Dataset/S04/bike4.csv path doesn t exist\n",
      "C:\\Projects\\Physical_activity_classifier/Smartphone_Dataset/S04/bike5.csv path doesn t exist\n",
      "C:\\Projects\\Physical_activity_classifier/Smartphone_Dataset/S06/bike1.csv path doesn t exist\n",
      "C:\\Projects\\Physical_activity_classifier/Smartphone_Dataset/S06/bike2.csv path doesn t exist\n",
      "C:\\Projects\\Physical_activity_classifier/Smartphone_Dataset/S06/bike3.csv path doesn t exist\n",
      "C:\\Projects\\Physical_activity_classifier/Smartphone_Dataset/S06/bike4.csv path doesn t exist\n",
      "C:\\Projects\\Physical_activity_classifier/Smartphone_Dataset/S06/bike5.csv path doesn t exist\n",
      "C:\\Projects\\Physical_activity_classifier/Smartphone_Dataset/S07/bike1.csv path doesn t exist\n",
      "C:\\Projects\\Physical_activity_classifier/Smartphone_Dataset/S07/bike2.csv path doesn t exist\n",
      "C:\\Projects\\Physical_activity_classifier/Smartphone_Dataset/S07/bike3.csv path doesn t exist\n",
      "C:\\Projects\\Physical_activity_classifier/Smartphone_Dataset/S07/bike4.csv path doesn t exist\n",
      "C:\\Projects\\Physical_activity_classifier/Smartphone_Dataset/S07/bike5.csv path doesn t exist\n",
      "C:\\Projects\\Physical_activity_classifier/Smartphone_Dataset/S07/gymbike5.csv path doesn t exist\n",
      "Cantidad de datos: 383\n",
      "Cantidad de caracteristicas: 27\n",
      "[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 7, 7, 7, 7, 8, 8, 8, 8, 8, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as skl\n",
    "import numpy as np\n",
    "import scipy as sci\n",
    "\n",
    "DIR='C:\\Projects\\Physical_activity_classifier/Smartphone_Dataset'\n",
    "data= pd.read_csv(DIR+'/S01/bike1.csv',header=None)\n",
    "#print(data)\n",
    "\n",
    "x=[]\n",
    "y=[]\n",
    "for suj in range(1,10):#######################10\n",
    "    for act in range(0,9):###################9\n",
    "        Activities=['bike','climbing','descending','gymbike','jumping','running','standing','treadmill','walking']\n",
    "        for k in range(1,6):\n",
    "            path=DIR+'/S0'+str(suj)+'/'+Activities[act]+str(k)+'.csv'\n",
    "            try:\n",
    "                #Activity[j].append(pd.read_csv(path,header=None))\n",
    "                df=pd.read_csv(path,header=None)\n",
    "                \n",
    "                #print(type(df))\n",
    "                data=df.as_matrix()\n",
    "                d_size=len(data)\n",
    "                #print(data)\n",
    "                final_data=[]\n",
    "                #Extract data for sensor\n",
    "                sensors=[[],[],[],[],[],[],[],[],[]]\n",
    "                for i in range(0,d_size):\n",
    "                    for j in range(0,9):\n",
    "                        sensors[j].append(data[i][j])\n",
    "\n",
    "                #---------------------mean-----------------------\n",
    "                ms=[]\n",
    "                for j in range(0,9):\n",
    "                    mean=np.mean(sensors[j])\n",
    "                    final_data.append(mean)\n",
    "                    ms.append(mean)\n",
    "                #print('mean:[{},{},{},{},{},{},{},{},{}]'.format(ms[0],ms[1],ms[2],ms[3],ms[4],ms[5],ms[6],ms[7],ms[8]))\n",
    "                #---------------------Var------------------------    \n",
    "                vs=[]\n",
    "                for j in range(0,9):\n",
    "                    var=np.var(sensors[j])\n",
    "                    final_data.append(var)\n",
    "                    vs.append(var)\n",
    "                #print('var:[{},{},{},{},{},{},{},{},{}]'.format(vs[0],vs[1],vs[2],vs[3],vs[4],vs[5],vs[6],vs[7],vs[8]))\n",
    "                #-------------------Entropy----------------------\n",
    "                Hs=[]\n",
    "                for j in range(0,9):\n",
    "                    H=sci.stats.entropy(sensors[j])\n",
    "                    #if(H==1):\n",
    "                    #    print('hola')\n",
    "                    #final_data.append(H)\n",
    "                    Hs.append(H)\n",
    "                #print('H:[{},{},{},{},{},{},{},{},{}]'.format(Hs[0],Hs[1],Hs[2],Hs[3],Hs[4],Hs[5],Hs[6],Hs[7],Hs[8]))\n",
    "                #-------------------Kurtosis----------------------\n",
    "                Ks=[]\n",
    "                for j in range(0,9):\n",
    "                    K=sci.stats.kurtosis(sensors[j])\n",
    "                    final_data.append(K)\n",
    "                    Ks.append(K)\n",
    "                #print('K:[{},{},{},{},{},{},{},{},{}]'.format(Ks[0],Ks[1],Ks[2],Ks[3],Ks[4],Ks[5],Ks[6],Ks[7],Ks[8]))\n",
    "                #-------------------Kurtosis----------------------\n",
    "                EEs=[]\n",
    "                for j in range(0,3):\n",
    "                    EE=0\n",
    "                    for i in range(0,d_size):\n",
    "                        EE+=(1/d_size)*np.sqrt(sensors[0+j*3][i]*sensors[0+j*3][i]+sensors[1+j*3][i]*sensors[1+j*3][i]+sensors[2+j*3][i]*sensors[2+j*3][i])\n",
    "                    #EE=sci.stats.kurtosis(sensors[j])\n",
    "                    #final_data.append(K)\n",
    "                    EEs.append(EE)\n",
    "                #print('K:[{},{},{}]'.format(EEs[0],EEs[1],EEs[2]))\n",
    "                \n",
    "                #-------------------Kurtosis----------------------\n",
    "                RMSs=[]\n",
    "                for j in range(0,9):\n",
    "                    RMS=0\n",
    "                    for i in range(0,d_size):\n",
    "                        RMS+=np.sqrt((1/d_size)*sensors[j][i]*sensors[j][i])\n",
    "                    #EE=sci.stats.kurtosis(sensors[j])\n",
    "                    #final_data.append(RMS)\n",
    "                    RMSs.append(RMS)\n",
    "                #print('RMS:[{},{},{}]'.format(RMSs[0],RMSs[1],RMSs[2]))\n",
    "                \n",
    "                y.append(act)\n",
    "                #print(final_data)\n",
    "                x.append(final_data)\n",
    "                \n",
    "            except FileNotFoundError :\n",
    "                print(path+' path doesn t exist')\n",
    "        \n",
    "    \n",
    "print('Cantidad de datos: {}'.format(len(x)))\n",
    "print('Cantidad de caracteristicas: {}'.format(len(x[0])))\n",
    "#print('Datos por actividad: [{},{},{},{},{},{},{},{},{}]'.format(len(Activity[0]),len(Activity[1]),len(Activity[2]),len(Activity[3]),len(Activity[4]),len(Activity[5]),len(Activity[6]),len(Activity[7]),len(Activity[8])))\n",
    "xtrain=x\n",
    "ytrain=y\n",
    "print(ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "def ANN_train(xdata,ydata,hl_size=10,activation='relu'):\n",
    "    clf = MLPClassifier(activation=activation,solver='lbfgs', hidden_layer_sizes=hl_size,\n",
    "    early_stopping = True, validation_fraction = 0.20);\n",
    "    X, X_val, y, y_val = train_test_split(xdata, np.ravel(ydata), test_size=clf.validation_fraction);\n",
    "    #print(y_val)\n",
    "    #print('size: {}'.format(len(y_val)))\n",
    "    \n",
    "    start = time.clock()\n",
    "    clf.fit(X, np.ravel(y))\n",
    "    end = time.clock()\n",
    "    dt=end-start\n",
    "    return [clf,X,X_val,y,y_val,dt]\n",
    "    \n",
    "\n",
    "def ANN_perf(xtest,ytest,clf):\n",
    "    ypred=clf.predict(xtest)\n",
    "    c_m=confusion_matrix(ytest, ypred)\n",
    "    return c_m\n",
    "\n",
    "def get_D(c_m):\n",
    "    dsum=0\n",
    "    for i in range(0,len(c_m)):\n",
    "        dsum+=c_m[i][i]\n",
    "    return dsum\n",
    "\n",
    "def get_Darray(c_m):\n",
    "    D=[]\n",
    "    for i in xrange(0,len(c_m)):\n",
    "        D.append(c_m[i][i])\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Neural Network\n",
      "-------------------------------------------------\n",
      "Matriz de Confusión:\n",
      "[[ 0  0  5  0  0  0  0  0  0]\n",
      " [ 0  6  1  0  0  0  0  0  0]\n",
      " [ 0  1  3  0  0  0  0  0  0]\n",
      " [ 1  0  0  7  0  0  0  0  0]\n",
      " [ 0  0  7  0  0  2  0  2  0]\n",
      " [ 0  0  8  1  1  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 11  0  0]\n",
      " [ 0  0  0  0  0  1  0 11  0]\n",
      " [ 0  0  7  0  0  1  0  0  1]]\n",
      "Clasificaciones exitosas: 39 de 77(50.6494%)\n",
      "Tiempo de entrenamiento: 0.35958161796224886s\n",
      "------------------------------------------------\n",
      "Matriz de Confusión:\n",
      "[[ 6  0  0  1  0  0  0  0  0]\n",
      " [ 0 10  1  0  0  0  0  0  0]\n",
      " [ 3  0  3  0  0  0  0  0  0]\n",
      " [ 1  0  0  4  0  0  0  0  0]\n",
      " [ 0  0  1  0  6  2  0  0  0]\n",
      " [ 0  0  1  0  2  6  0  1  1]\n",
      " [ 1  0  0  1  0  0  7  0  0]\n",
      " [ 0  0  0  0  0  0  0 10  0]\n",
      " [ 1  0  1  0  1  0  0  0  6]]\n",
      "Clasificaciones exitosas: 58 de 77(75.3247%)\n",
      "Tiempo de entrenamiento: 7.502268221214763s\n",
      "------------------------------------------------\n",
      "Matriz de Confusión:\n",
      "[[ 5  0  0  2  0  0  0  0  0]\n",
      " [ 0  9  0  0  0  1  0  0  0]\n",
      " [ 0  0  3  0  0  0  0  0  0]\n",
      " [ 0  0  0 11  0  0  1  0  0]\n",
      " [ 0  0  0  0  8  5  0  0  2]\n",
      " [ 0  0  0  0  4  7  0  0  0]\n",
      " [ 0  0  1  0  0  0 10  0  0]\n",
      " [ 0  0  0  0  0  0  0  3  1]\n",
      " [ 2  0  0  0  0  0  0  0  2]]\n",
      "Clasificaciones exitosas: 58 de 77(75.3247%)\n",
      "Tiempo de entrenamiento: 36.20409813151127s\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#hl_size_op=5000#(28+10)//2\n",
    "hl_s=[10,1000,5000]\n",
    "\n",
    "print('-------------------------------------------------')\n",
    "print('Neural Network')\n",
    "print('-------------------------------------------------')\n",
    "for i in range(0,3):\n",
    "    result=ANN_train(xtrain,ytrain,hl_size=hl_s[i])\n",
    "    c_m=ANN_perf(result[2],result[4],result[0])\n",
    "    print('Matriz de Confusión:')\n",
    "    print(c_m)\n",
    "    dsum=get_D(c_m)\n",
    "    percentage=np.round((dsum/(len(result[4])*1.0))*100,4)\n",
    "    print('Clasificaciones exitosas: {} de {}({}%)'.format(dsum,len(result[4]),percentage))\n",
    "    print('Tiempo de entrenamiento: {}s'.format(result[5]))\n",
    "    print('------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pol(xtrain,ytrain,deg=3):\n",
    "    std_scale=skl.preprocessing.StandardScaler().fit(xtrain)\n",
    "    df_std = std_scale.transform(xtrain)\n",
    "    clf = SVC(kernel='poly',degree=deg)\n",
    "    clf.fit(df_std, ytrain)\n",
    "    return clf  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Support Vector Machine\n",
      "-------------------------------------------------\n",
      "Matriz de Confusión:\n",
      "[[ 0  1  2  0  0  0  5  0 22]\n",
      " [ 0 42  2  0  0  0  0  0  1]\n",
      " [ 0  1 22  0  0  0  5  0 17]\n",
      " [ 0  1  2  0  0  0  0  0 36]\n",
      " [ 0 12  7  0  1  1  1  0 23]\n",
      " [ 0 15  9  0  0  1  0  0 20]\n",
      " [ 0  0  0  0  0  0 29  0 16]\n",
      " [ 0 14  0  0  0  2  0  0 28]\n",
      " [ 0  9  2  0  0  0  3  0 31]]\n",
      "Clasificaciones exitosas: 126 de 383(32.8982%)\n",
      "-------------------------------------------------\n",
      "Matriz de Confusión:\n",
      "[[14  8  6  0  0  0  0  2  0]\n",
      " [ 0 44  0  0  1  0  0  0  0]\n",
      " [ 3  4 38  0  0  0  0  0  0]\n",
      " [32  1  2  1  0  0  3  0  0]\n",
      " [ 0 16 11  0  9  0  0  8  1]\n",
      " [ 0 13 11  0  6  1  0 13  1]\n",
      " [ 8  5  5  4  0  0 20  3  0]\n",
      " [ 0  2  0  0  0  0  0 42  0]\n",
      " [ 1 14 10  0  7  0  0  6  7]]\n",
      "Clasificaciones exitosas: 176 de 383(45.953%)\n",
      "-------------------------------------------------\n",
      "Matriz de Confusión:\n",
      "[[12  3 14  0  1  0  0  0  0]\n",
      " [ 0 44  0  0  1  0  0  0  0]\n",
      " [ 0  4 41  0  0  0  0  0  0]\n",
      " [27  0  7  1  0  0  2  0  2]\n",
      " [ 0 15  7  0 12  0  0  8  3]\n",
      " [ 0 11  9  0 11  1  0 13  0]\n",
      " [ 3  4  8  8  0  0 16  6  0]\n",
      " [ 0  2  5  0  0  0  0 37  0]\n",
      " [ 0 11 13  0 17  0  0  4  0]]\n",
      "Clasificaciones exitosas: 164 de 383(42.8198%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC,LinearSVC\n",
    "def svm_poly(xtrain,ytrain,xtest,ytest,deg=3):\n",
    "    #Entrenamiento\n",
    "    std_scale=skl.preprocessing.StandardScaler().fit(xtrain)\n",
    "    df_std = std_scale.transform(xtrain)\n",
    "    clf = SVC(kernel='linear',C=deg)\n",
    "    clf.fit(df_std, ytrain)  \n",
    "    #Validacion\n",
    "    std_scale=skl.preprocessing.StandardScaler().fit(xtest)\n",
    "    feattest = std_scale.transform(xtest)\n",
    "    #score = clf_p.decision_function(feattest)\n",
    "    #ypred=clf.predict(feattest)\n",
    "    \n",
    "    c_m=ANN_perf(xtest,ytest,clf)\n",
    "    print('Matriz de Confusión:')\n",
    "    print(c_m)\n",
    "    dsum=get_D(c_m)\n",
    "    percentage=np.round((dsum/(len(ytest)*1.0))*100,4)\n",
    "    print('Clasificaciones exitosas: {} de {}({}%)'.format(dsum,len(ytest),percentage))\n",
    "    #print('Tiempo de entrenamiento: {}s'.format(result[5]))\n",
    "    \n",
    "print('-------------------------------------------------')\n",
    "print('Support Vector Machine')\n",
    "print('-------------------------------------------------')\n",
    "svm_poly(xtrain,ytrain,xtrain,ytrain,deg=0.0001)\n",
    "print('-------------------------------------------------')\n",
    "svm_poly(xtrain,ytrain,xtrain,ytrain,deg=1)\n",
    "print('-------------------------------------------------')\n",
    "svm_poly(xtrain,ytrain,xtrain,ytrain,deg=1000)\n",
    "#svm_poly(Mtrain,Mval,deg=2)\n",
    "#svm_poly(Mtrain,Mval,deg=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
